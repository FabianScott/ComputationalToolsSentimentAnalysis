{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "No outputs in this section, as it was run on the HPC as regular python scripts, not a notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import mmh3\n",
    "import time\n",
    "import fasttext\n",
    "import numpy as np\n",
    "from copy import copy\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, \\\n",
    "    Birch, MiniBatchKMeans, SpectralClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "def vectorise_text(review_list, fasttext_model, create_file=False, name='Train'):\n",
    "    \"\"\"\n",
    "    Given a list of ratings and reviews and a fasttext model, this function\n",
    "    returns the average of the word vectors as represented by the model.\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    counter = 0\n",
    "    for review in review_list:\n",
    "        if counter % 1000 == 1:\n",
    "            print(f'{np.round(100 * counter / len(review_list), 6)} % done')\n",
    "        text_list = []\n",
    "        for word in review[1]:\n",
    "            temp_vector = fasttext_model.get_word_vector(word)\n",
    "            text_list.append(temp_vector)\n",
    "        output.append(np.sum(np.array(text_list), axis=0) / len(text_list))\n",
    "        counter += 1\n",
    "    output = [[review_list[i][0], el] for i, el in enumerate(output)]\n",
    "    if create_file:\n",
    "        df = pd.DataFrame(output)\n",
    "        df.to_csv(f'{name}_fasttext_vectors.csv', header=True, index=False)\n",
    "    return output\n",
    "\n",
    "\n",
    "def minhash_text(review_list, q=9, minhash_length=100, create_file=False, name=''):\n",
    "    \"\"\"\n",
    "    Given the review list including the star ratings, return a list\n",
    "    of each review minhashed. The seed and length of minhash vector\n",
    "    can be specified.\n",
    "\n",
    "    :param name:\n",
    "    :param create_file:\n",
    "    :param review_list:         contains [stars, review_text]\n",
    "    :param q:\n",
    "    :param minhash_length:\n",
    "    :param seed:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    counter = 0\n",
    "    for review in review_list:\n",
    "        if counter % 1000 == 1:\n",
    "            print(f'{np.round(100 * counter / len(review_list), 6)} % done')\n",
    "        shingles_list = q_shingles(review[1], q)\n",
    "        output.append(minhash(shingles_list, k=minhash_length, stars=int(review[0])))\n",
    "        counter += 1\n",
    "    if create_file:\n",
    "        df = pd.DataFrame(output, dtype=int)\n",
    "        df.to_csv(f'{name}_minhash_vectors.csv', header=True, index=False)\n",
    "    return output\n",
    "\n",
    "\n",
    "def minhash(shingles_list, k=1, stars=None):\n",
    "    \"\"\"\n",
    "    Given list of shingles of words representing a text, compute the\n",
    "    list of min-hashes of length k for that text, can add star rating\n",
    "    as the first column if specified\n",
    "    :param stars:\n",
    "    :param shingles_list:\n",
    "    :param k:\n",
    "    :return minh:\n",
    "    \"\"\"\n",
    "    minh = []\n",
    "    if stars is not None:\n",
    "        minh.append(stars)\n",
    "\n",
    "    for i in range(k):  # define the seeds for the hash function\n",
    "        temp = np.inf\n",
    "        if shingles_list:\n",
    "            for shingle in shingles_list:\n",
    "                temp = min(temp, mmh3.hash(shingle, seed=i, signed=False))\n",
    "            minh.append(int(temp))\n",
    "        else:\n",
    "            minh.append(0)\n",
    "    return minh\n",
    "\n",
    "\n",
    "def q_shingles(string, q, characters=True):\n",
    "    \"\"\"\n",
    "    Given string and length of shingles, returns a set of\n",
    "    all shingles of characters/words in the string.\n",
    "    :param characters:\n",
    "    :param string:\n",
    "    :param q:\n",
    "    :return output:\n",
    "    \"\"\"\n",
    "    output = set()\n",
    "    if characters:\n",
    "        length = len(string)\n",
    "        for i in range(length - q):\n",
    "            output.add(string[i:i + q])\n",
    "    else:\n",
    "        string_list = string.split()\n",
    "        length = len(string_list)\n",
    "        for i in range(length - q):\n",
    "            output.add(tuple(string[i:i + q]))\n",
    "    return output\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run fasttext, save the model with the test accuracy in title"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = fasttext.train_supervised('train_fasttext.txt', autotuneValidationFile='test_fasttext.txt', autotuneDuration=600)\n",
    "res = model.test('test_fasttext.txt')\n",
    "model.save_model(f'autotuned_apparel{res[1]}.bin')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create Minhash vectors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "data_path = os.path.join(os.getcwd(), 'data')\n",
    "train_list, test_list = load_review_lists(filepath=data_path)\n",
    "print(f'Loading data took: {time.time() - t}')\n",
    "t = time.time()\n",
    "\n",
    "minhash_text(test_list, create_file=True, name='testaaaa')\n",
    "print(f'Vectorising test data took: {time.time() - t}')\n",
    "\n",
    "minhash_text(train_list, create_file=True, name='trainaaaa')\n",
    "print(f'Vectorising training data took: {time.time() - t}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create Fasttext vectors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "data_path = os.path.join(os.getcwd(), 'data')\n",
    "train_list, test_list = load_review_lists(filepath=data_path)\n",
    "model_path = 'autotuned_apparel1_0.552265.bin'\n",
    "model = fasttext.load_model(model_path)\n",
    "print(f'Loading data took: {time.time() - t}')\n",
    "t = time.time()\n",
    "\n",
    "vectorise_text(train_list, fasttext_model=model, create_file=True, name='train1')\n",
    "print(f'Vectorising training data took: {time.time() - t}')\n",
    "\n",
    "vectorise_text(test_list, fasttext_model=model, create_file=True, name='test1')\n",
    "print(f'Vectorising test data took: {time.time() - t}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run random classification"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filename_list = ['train_fasttext', 'train_minhash', 'test_fasttext', 'test_minhash']\n",
    "path_list = [os.path.join(os.getcwd(), f'{name}_vectors.csv') for name in filename_list]\n",
    "\n",
    "n_test = None\n",
    "\n",
    "# Loading Test data\n",
    "t = time.time()\n",
    "ft_test_v, ft_test_r = load_review_vectors(path_list[2], no_reviews=n_test)\n",
    "mh_test_v, mh_test_r = load_review_vectors(path_list[3], no_reviews=n_test)\n",
    "print(f'Loading test data took: {time.time() - t}')\n",
    "print(f'Shape of test data:\\nft: {ft_test_v.shape}\\nmh: {mh_test_v.shape}')\n",
    "\n",
    "np.random.seed(42)\n",
    "ft_true = sum((ft_test_r - np.random.randint(1, 6, size=ft_test_r.shape)) == 0)/len(ft_test_r)\n",
    "mh_true = sum((mh_test_r - np.random.randint(1, 6, size=mh_test_r.shape)) == 0)/len(mh_test_r)\n",
    "print(f'Proportion correct on Fasttext: {ft_true}')\n",
    "print(f'Proportion correct on Minhash:  {mh_true}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define functions for clustering"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def clustering(points, method='k_means', homemade=False, k=5, centroids=None, tol=1e-5, show_cluster=False,\n",
    "               title='Clusters Shown', birch_thresh=0.01):\n",
    "    \"\"\"\n",
    "    K-means clustering using either the homemade implementation or sk-learn's.\n",
    "    It can show a 2D plot of the first two dimensions of the clusters.\n",
    "    In the homemade version the centroids can be specified.\n",
    "    :param method:\n",
    "    :param points:\n",
    "    :param homemade:\n",
    "    :param k:\n",
    "    :param centroids:\n",
    "    :param tol:\n",
    "    :param show_cluster:\n",
    "    :param title:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    points = np.array(points)\n",
    "    if method[0].lower() == 'k':\n",
    "        if homemade:\n",
    "            if centroids is None:\n",
    "                random_indices = np.random.randint(0, len(points), k)\n",
    "                centroids = np.array([points[i] for i in random_indices])\n",
    "\n",
    "            cluster_assignments = np.zeros(len(points), dtype=int)\n",
    "            temp = np.ones(centroids.shape) - centroids\n",
    "            while np.array([el > tol for el in np.abs(centroids - temp)]).any():\n",
    "                temp = copy(centroids)\n",
    "                # cluster_assignments = [np.argmin(np.sum(np.abs(centroids - point), axis=1)) for i, point in enumerate(points)]\n",
    "                for i, point in enumerate(points):\n",
    "                    distances = np.sum(np.abs(centroids - point), axis=1)\n",
    "                    cluster_assignments[i] = int(np.argmin(distances))\n",
    "                for i in range(k):\n",
    "                    ci_points = points[cluster_assignments == i]\n",
    "                    centroids[i] = np.sum(ci_points, axis=0) / len(ci_points)\n",
    "        else:\n",
    "            model = KMeans(n_clusters=k, init='k-means++').fit(points)\n",
    "            cluster_assignments = model.labels_\n",
    "    elif method[0].lower() == 'a':\n",
    "        model = AgglomerativeClustering(n_clusters=k).fit(points)\n",
    "        cluster_assignments = model.labels_\n",
    "    elif method[0].lower() == 'b':\n",
    "        model = Birch(n_clusters=k, threshold=birch_thresh).fit(points)\n",
    "        cluster_assignments = model.labels_\n",
    "    elif method[0].lower() == 'm':\n",
    "        model = MiniBatchKMeans(n_clusters=k, init='k-means++').fit(points)\n",
    "        cluster_assignments = model.labels_\n",
    "    elif method[0].lower() == 's':\n",
    "        model = SpectralClustering(n_clusters=k).fit(points)\n",
    "        cluster_assignments = model.labels_\n",
    "    elif method[0].lower() == 'g':\n",
    "        model = GaussianMixture(n_components=k, init_params='k-means++').fit(points)\n",
    "        cluster_assignments = model.predict(points)\n",
    "\n",
    "    if show_cluster:\n",
    "        show_clustering(points, cluster_assignments, title=title)\n",
    "\n",
    "    return cluster_assignments, model\n",
    "\n",
    "\n",
    "def show_clustering(points, assignments, title='Clusters shown'):\n",
    "    points = np.array(points)\n",
    "    if points.shape[1] > 2:\n",
    "        print('Show_cluster will only show the first 2 dimensions of points!!')\n",
    "    cmap = {0: 'b', 1: 'y', 2: 'g', 3: 'r', 4: 'm', 5: 'c', 6: 'k'}\n",
    "    try:\n",
    "        color_list = [cmap[el] for el in assignments]\n",
    "    except ValueError:\n",
    "        print('Only 7 colours are available, plot failed')\n",
    "        raise ValueError\n",
    "    plt.scatter(points[:, 0], points[:, 1], c=color_list)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def cluster_closeness_matrix(true_labels, clusters, decimals=3):\n",
    "    \"\"\"\n",
    "    Calculate the percentage of labels corresponding to each true label\n",
    "    in each cluster.\n",
    "    \"\"\"\n",
    "    k = len(np.unique(true_labels))  # number of labels\n",
    "    cluster_closeness_mat = []\n",
    "    weights = [sum(true_labels == i) for i in range(1, k + 1)]\n",
    "    for i in range(k):  # for every label\n",
    "        # j is star ratings so between 1-5.\n",
    "        counts = [np.sum(true_labels[clusters == i] == j) for j in range(1, k + 1)]\n",
    "\n",
    "        if any(counts):  # avoid division by 0\n",
    "            cluster_closeness_mat.append(counts / sum(counts))\n",
    "        else:\n",
    "            cluster_closeness_mat.append([0 for _ in range(k)])\n",
    "    cluster_closeness_mat = np.round(np.array(cluster_closeness_mat), decimals)\n",
    "    return cluster_closeness_mat, weights\n",
    "\n",
    "\n",
    "def assign_clusters(m, label_counts=None):\n",
    "    if label_counts is None:\n",
    "        label_counts = np.ones(len(m))\n",
    "\n",
    "    real_combs = []\n",
    "    # Create all combinations of mappings:\n",
    "    comb_mat = combinations([(i, j) for i in range(5) for j in range(5)], 5)\n",
    "    for el in list(comb_mat):\n",
    "        temp = np.array(el)\n",
    "        if len(np.unique(temp[:, 0])) == 5 and len(np.unique(temp[:, 1])) == 5:\n",
    "            real_combs.append(el)\n",
    "    # Calculate the proportion correct from each combination\n",
    "    max_i, max_sum = 0, 0\n",
    "    for i, coord_set in enumerate(real_combs):\n",
    "        temp_sum = 0\n",
    "        for j, coord in enumerate(coord_set):\n",
    "            temp_sum += m[coord] * label_counts[j]\n",
    "        if temp_sum > max_sum:\n",
    "            max_sum = temp_sum\n",
    "            max_i = i\n",
    "    # Find the maximum combination\n",
    "    assignments = np.array(real_combs[max_i])[:, 1] + 1\n",
    "    a_dict = {}\n",
    "    # Index is cluster, element is star rating\n",
    "    for i, el in enumerate(assignments):\n",
    "        a_dict[i] = el\n",
    "    return a_dict\n",
    "\n",
    "\n",
    "def p_correct_clusters(true_labels, test_vectors, cluster_map, assigned_labels=None, train_vectors=None, knn=15,\n",
    "                       model=None):\n",
    "    \"\"\"\n",
    "    Given the test vectors, and vectors on which the cluster was trained\n",
    "    find the knn of each point and use majority voting to assign this\n",
    "    a label. Count the number of correctly assigned labels and return a\n",
    "    proportion of correctly labelled points.\n",
    "    :param cluster_map:\n",
    "    :param test_vectors:\n",
    "    :param train_vectors:\n",
    "    :param assigned_labels:\n",
    "    :param true_labels:\n",
    "    :param knn:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    correct, incorrect = 0, 0\n",
    "    if model is None:\n",
    "        for idx, point in enumerate(test_vectors):\n",
    "            distances = np.sum(np.abs(point - train_vectors), axis=1)\n",
    "            label = np.bincount(\n",
    "                assigned_labels[np.argpartition(distances, knn)[:knn]]).argmax()  # chose lowest if tied\n",
    "            correct += label == true_labels[idx]\n",
    "    else:\n",
    "        incorrect = np.count_nonzero(true_labels - np.array([cluster_map[el] for el in model.predict(test_vectors)]))\n",
    "    return correct / len(true_labels) if correct else (1 - incorrect / len(true_labels))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run clustering"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "knn, k = 9, 5\n",
    "n_train, n_test = 1000, 100\n",
    "cluster_types = ['K-means', 'Minibatch-Kmeans']     # , 'Gaussian-Mixture' , 'Agglomerative', 'Birch', 'Spectral']\n",
    "\n",
    "\n",
    "filename_list = ['train_fasttext', 'train_minhash', 'test_fasttext', 'test_minhash']\n",
    "path_list = [os.path.join(os.getcwd(), f'{name}_vectors.csv') for name in filename_list]\n",
    "\n",
    "# Loading Training Data\n",
    "t = time.time()\n",
    "ft_train_v, ft_train_r = load_review_vectors(path_list[0], no_reviews=n_train)\n",
    "mh_train_v, mh_train_r = load_review_vectors(path_list[1], no_reviews=n_train)\n",
    "print(f'Loading training data took: {time.time() - t}')\n",
    "print(f'Shape of training data:\\nft: {ft_train_v.shape}\\nmh: {mh_train_v.shape}')\n",
    "# Loading Test data\n",
    "t = time.time()\n",
    "ft_test_v, ft_test_r = load_review_vectors(path_list[2], no_reviews=n_test)\n",
    "mh_test_v, mh_test_r = load_review_vectors(path_list[3], no_reviews=n_test)\n",
    "print(f'Loading test data took: {time.time() - t}')\n",
    "print(f'Shape of test data:\\nft: {ft_test_v.shape}\\nmh: {mh_test_v.shape}')\n",
    "\n",
    "\n",
    "proportion_correct, cluster_assignments, cc_mats, models, weights = [], [], [], [], []\n",
    "for i, name in enumerate(cluster_types):\n",
    "    # Run the clustering\n",
    "    t = time.time()\n",
    "    labels_ft, model_ft = clustering(ft_train_v, method=name)\n",
    "    print(f'{name} took: {time.time() - t} seconds on fasttext')\n",
    "    labels_mh, model_mh = clustering(mh_train_v, method=name)\n",
    "    print(f'{name} took: {time.time() - t} seconds on minhash')\n",
    "\n",
    "    t = time.time()\n",
    "    # Proportion of each class in the clusters, each row is a cluster column is star rating\n",
    "    m1, w1 = cluster_closeness_matrix(ft_train_r, labels_ft, decimals=4)\n",
    "    m2, w2 = cluster_closeness_matrix(mh_train_r, labels_mh, decimals=4)\n",
    "\n",
    "    # Using the maximum proportions assign each cluster a star rating, creates a dict:\n",
    "    label_map_ft, label_map_mh = assign_clusters(m1, w1), assign_clusters(m2, w2)\n",
    "    # Use predict method and compare to the assigned clusters\n",
    "    correct_proportion_ft = p_correct_clusters(ft_test_r, ft_test_v, label_map_ft, model=model_ft)\n",
    "    correct_proportion_mh = p_correct_clusters(mh_test_r, mh_test_v, label_map_mh, model=model_mh)\n",
    "    print(f'Calculating the correct proportion took {time.time()-t} seconds')\n",
    "\n",
    "    # Append all desired data to corresponding lists\n",
    "    cluster_assignments.append((label_map_ft, label_map_mh))\n",
    "    cc_mats.append((m1, m2))\n",
    "    models.append((model_ft, model_mh))\n",
    "    weights.append((w1, w2))\n",
    "    proportion_correct.append((correct_proportion_ft, correct_proportion_mh))\n",
    "\n",
    "print(proportion_correct)\n",
    "print(f'The proportion of each class in each cluster is: {weights}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}